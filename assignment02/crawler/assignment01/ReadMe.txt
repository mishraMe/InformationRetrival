I am coding in Python, I have used:
- BeautifulSoup libarary, 
- time, 
 -urllib2 to complete this assignment.

The tasks have their individual files for task1 and task2 respectively. 

1. crawlerTask1.py
Task 1 accepts input of one argument that is the seedUrl
when you normally run the file crawlerTask1.py it accepts the seed url that you want to crawl
the url must be provided in ""
for example : enter seed url "https://en.wikipedia.org/wiki/Solar_power"
For files
2. crawlerTask2BFS.py
3. crawlerTask3.py
The programs do not use any argument.
The seed has been included as a constant "seedUrl" in the file itself.

Also the files are saved in particular files.
1. crawlerTask1.py  has:
i) a "sustainableEnergyLinks" variable that saves the name of the txt file that
saves the output crawled pages list.
ii) and "directory_name" variable that saves the location of directory to save the generated docs
in the task 1.
2. crawlerTask2BFS.py has :
i) a "sustainableEnergyLinks" variable that saves the name of the txt file that
saves the output crawled pages list.
3. crawlerTask2DFS.py has:
i) a "sustainableEnergyLinks" variable that saves the name of the txt file that
saves the output crawled pages list.


Apart from the code
there are 4 files. 
1. task_1_links - containing links for sustainable energy seed link generated by task1 program

2. task_2_BFS - containing links for sustainable enegrgy seed link generated by task2 for searching "solar" by BFS approach

3. task_2_DFS -containing links for sustainable enegrgy seed link generated by task2 for searching "solar" by DFS approach

4. task3_links - containing links for solar power seed link generated by task1 program.
and text files for 

NOTE: Please note that the crawlers for task1 and task2-BFS take a bit longer time to run and the text printed on console is just for debugging perpose. It doesn't output much as I thought generating a lot of print outputs would take longer time.

Also, DFS has been unsuccessfully implemented and I guess it loops in the end. It is working fine for depth 1 i.e., if the depth was 1 it gives expected output but it runs into a loop for the other depths. 


The answer to Task2-Part-C is thus a logical explaination and the top 5 links for DFS are based on the hypothesis that if output it generated for depth 1 (given in file task_2_DFS_for_depth_1.txt)is in the perfect order that the seedURL is crawled last and the sibling nodes are crawled in the order left to right.
then the same would be the case if it was implimented correctly. (important note here is that the relevence decreases as we go from left to right i.e., leftmost child is more relevant than the rightmost child)


